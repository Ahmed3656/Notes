NoSQL databases emerged in the 2000s to handle web-scale data and flexible JSON documents. Unlike SQL databases, which use rigid tables and a SQL API, NoSQL systems store data as documents, graphs, key-values, or columnar formats and use custom get/set-style APIs. However, at the storage layer, both SQL and NoSQL databases share common fundamentals: data pages, indexes, transactions, write-ahead logging (WAL), and compression all aimed at minimizing disk I/O.

<hr class="hr-light" />

#### **Core Components of a Database**
- **Front end (API)**: How applications interact with the database (e.g., SQL, MongoDB commands).
- **Storage engine**: How data is stored and retrieved from disk (handles pages, indexes, WAL, locking, etc.).

NoSQL databases differ mainly in data model and API, not in storage engine concerns.

---

### **MongoDB's Architecture**
MongoDB is a popular NoSQL document-oriented database that uses JSON-like documents with optional schemas. Internally, these documents are stored in a more compact and efficient binary format called **BSON (Binary JSON)**. Documents are then grouped into **collections**, which are similar to tables in a relational database.

Every document in a MongoDB collection has a unique `_id` field, which is automatically indexed with a **[B-Tree](../Data%20Structures/B-Trees.md)** to allow for fast lookups.

<hr class="hr-light" />

#### **Key MongoDB Concepts**

- **BSON**: Binary JSON, faster encoding/decoding than JSON.
- **`_id` field**: Default 12-byte ObjectId (timestamp + machine ID + process ID + counter). Users can override with custom values.
- **Indexes**: Critical for performance. If indexes don’t fit in memory —> disk paging —> performance collapse (e.g., Discord moved to Cassandra for this reason).

<hr class="hr-light" />

#### **The Evolution of MongoDB's Storage Engine**
The way MongoDB stores and retrieves data has evolved over time:
- **MMAPv1 (Memory Mapped Files):** this was MongoDB’s original storage engine, which relied on **memory-mapped files** to store BSON documents. It used a **DiskLoc** value (file ID + byte offset) to point directly to the document on disk, allowing **direct lookups** like: `_id index —> DiskLoc —> document`
	**Strengths:**
	- Simple and efficient for reads.
	- Direct mapping meant predictable lookup paths.
	**Limitations:**
	- **Document updates** that changed document size could break offsets, causing inefficient writes.
	- **Locking was coarse-grained**: initially a global database lock, later improved to collection-level.
	
	Overall: great for reads, poor for concurrent writes.
- **WiredTiger:** introduced in 2014 as the new default storage engine, WiredTiger brought major upgrades that solved some of the limitations of MMAPv1. It introduced:
    - **Document-level locking:** This allows for true concurrent writes.
    - **BSON compression:** This reduces disk I/O and increases memory efficiency.
    - **Clustered B-Tree with hidden RecordId** (8-byte integer) for document storage.
    - A new index model based on a `recordID`.
    **Lookup path:** `_id index —> RecordId —> hidden index —> document`
	
	While WiredTiger improved concurrency and efficiency, fetching a document by its `_id` still required two lookups: one to find the `recordID` from the index, and a second to fetch the document using that `recordID`.

<hr class="hr-light" />

#### **How Clustered Collections Revolutionized Lookups**
Building on WiredTiger’s improvements, **clustered collections**, introduced in **MongoDB 5.3**, solved the extra-lookup problem by making the `_id` field the **clustered index**, storing documents directly in the leaf nodes of the `_id` B-Tree, thereby co-locating each document with its `_id`.

**Performance Benefits:**
- **Faster Queries:** Fetching a document by `_id` now requires only a single index lookup (O(log n)), instead of two.
- **Efficient Range Scans:** Leaf pages are sequentially linked, making `_id` range queries faster.
- **Lower Storage Overhead:** Compression and locality reduce storage usage, improving insert and bulk insert performance.
**Trade-offs:**
- Secondary indexes now store the `_id` value instead of the compact 8-byte RecordId.
- Large `_id` values (UUIDs, long strings) can increase secondary index size —> more storage, potentially slower I/O.

<hr class="hr-light" />

#### **How to Implement a Clustered Collection in MongoDB**

```javascript
// Create a clustered collection at creation time
db.createCollection("users", {
  clusteredIndex: {
    key: { _id: 1 },
    unique: true,
    name: "clustered_users"
  }
});

// Insert document
db.users.insert({ _id: ObjectId(), name: "Alice", age: 30 });

// Query by _id - now only one lookup
db.users.find({ _id: ObjectId("...") });
```

<hr class="hr-light" />

#### **Impact on Secondary Indexes (Major Consideration)**
While clustered collections provide a big performance boost for `_id` lookups, there is a critical trade-off to consider when using secondary indexes.
- **Regular Collections:** in a regular collection, a secondary index stores an 8-byte `recordID` as its pointer to the document.
- **Clustered Collections:** in a clustered collection, the secondary index must store the value of the `_id` field as its pointer.

This has a significant consequence: if your `_id` is large (for example, a 36-byte UUID string instead of the default 12-byte ObjectId), all your secondary indexes will become much larger. This increases storage requirements and can degrade I/O performance for queries using those secondary indexes, as fewer index entries will fit on a single disk page.

---

### **Cassandra’s Architecture**
Cassandra is another major NoSQL system, but instead of documents, it is a **wide-column store**. It was originally built at Facebook to handle massive inbox search at web scale, and it is designed to run reliably across **many nodes** without a single point of failure.

Unlike MongoDB, which looks like JSON documents grouped into collections, Cassandra organizes data into **keyspaces** (like databases) and **tables** (like SQL tables, but more flexible). Each row in a Cassandra table is defined by a **partition key** (which decides where the data lives in the cluster) and optional **clustering columns** (which define the on-disk sort order inside a partition).

So basically:
- The **partition key** decides which machine gets the data.
- The **clustering columns** decide how the rows are ordered on disk within that partition.

This design is critical because it ensures scalability and predictable read/write performance across large clusters.

<hr class="hr-light" />

#### **Key Cassandra Concepts**

- **Partition Key**: defines the data distribution. For example, if you pick `user_id` as the partition key, then all activity for a given user will live together on the same node. That’s why good partition key design is essential for load balancing.
- **Clustering Columns**: specify the order of rows inside a partition. So the data is physically sorted on disk by these columns, which makes range queries inside a partition fast.
- **Tunable Consistency**: Cassandra lets you decide consistency per query (`ONE`, `QUORUM`, `ALL`). You can trade availability for stronger guarantees when you need them.
- **Query Language (CQL)**: looks like SQL but is simpler. For example, you can `SELECT` rows, but you cannot do arbitrary joins. This ensures queries map efficiently to the underlying storage model.

<hr class="hr-light" />

#### **How Cassandra Stores and Retrieves Data**
Unlike MongoDB’s [B-Tree](../Data%20Structures/B-Trees.md) approach, Cassandra is built on **[LSM-Trees](../Data%20Structures/LSM%20Tree%20(Log-Structured%20Merge%20Tree).md)**.

- **Writes**: When you insert or update, Cassandra writes to a **commit log** (WAL) to ensure durability, and then to an in-memory structure called a **MemTable**. Once the MemTable is full, it is flushed to disk as an **SSTable** (Sorted String Table).
- **Reads**: When you query, Cassandra checks the MemTable first, then Bloom filters to quickly see if an SSTable might contain the row, then pulls data from the relevant SSTables. A **compaction process** runs in the background to merge SSTables, remove old versions, and reclaim space.
- **Bloom Filters**: Lightweight probabilistic data structures that act as a fast check: “Is it possible that this SSTable contains the row?” If not, Cassandra skips that SSTable, which makes reads efficient.

So the lookup path is: **partition key —> node —> MemTable/Bloom filters —> SSTables —> row(s)**.

<hr class="hr-light" />

#### **Strengths and Limitations**

**Strengths:**
- **Horizontal Scalability:** Cassandra is designed to scale linearly. If you add more nodes, you get more throughput, no central bottleneck.
- **High Availability:** because it uses peer-to-peer replication with no master node, Cassandra keeps running even if some nodes are down.
- **Fast Writes:** the LSM-Tree design makes inserts and updates lightweight and sequential, which is ideal for write-heavy workloads.
- **Range Queries within Partitions:** since clustering columns define sort order, range scans inside a partition are efficient.

**Limitations:**
- **Read Amplification:** because data may exist in multiple SSTables before compaction, reads can be slower compared to write-heavy workloads.
- **Complex Data Modeling:** you need to design tables around queries up front. Unlike MongoDB’s flexible schema, Cassandra forces you to think about partition keys and clustering keys in advance.
- **Limited Joins & Aggregations:** CQL looks like SQL, but you cannot do complex joins or aggregations directly. You usually push that logic into the application or use Spark.

<hr class="hr-light" />

#### **Creating a Table in Cassandra**
```sql
-- Create keyspace
CREATE KEYSPACE myapp WITH REPLICATION = {
  'class': 'SimpleStrategy',
  'replication_factor': 3
};

-- Create table with partition key (user_id) and clustering column (timestamp)
CREATE TABLE user_activity (
  user_id uuid,
  activity_time timestamp,
  activity_type text,
  PRIMARY KEY (user_id, activity_time)
) WITH CLUSTERING ORDER BY (activity_time DESC);

-- Insert data
INSERT INTO user_activity (user_id, activity_time, activity_type)
VALUES (uuid(), toTimestamp(now()), 'login');

-- Query efficiently by partition key and clustering column
SELECT * FROM user_activity WHERE user_id = uuid() LIMIT 10;
```

Here, the partition key (`user_id`) ensures all activity for a given user is stored on the same node, while the clustering column (`activity_time`) keeps rows sorted, so you can quickly get “latest N activities” without scanning the whole table.

<hr class="hr-light" />

#### **How Cassandra Compares to MongoDB**

- **Data Model:** MongoDB stores flexible JSON-like documents, while Cassandra stores rows grouped into sorted partitions. Mongo makes it easy to change structure on the fly, but Cassandra forces you to define partition and clustering keys up front, which can be harder to model but ensures predictable performance.
- **Indexes:** MongoDB relies heavily on B-Trees and secondary indexes. Cassandra mostly depends on partition keys + clustering columns for efficient access. It does have secondary indexes, but they are less efficient than Mongo’s.
- **Consistency:** MongoDB ensures strong consistency within a replica set by default. Cassandra gives you tunable consistency per query (for example `ONE`, `QUORUM`, or `ALL`), so you can choose between faster responses or stricter guarantees.
- **Write Path:** Mongo uses a WAL + B-Trees; Cassandra uses WAL + MemTable + SSTables with compaction. The LSM-Tree design makes Cassandra much better for large-scale, write-heavy workloads, while Mongo is more balanced between reads and writes.
- **Read Path:** Mongo reads are usually faster and more straightforward, especially with clustered collections, while Cassandra reads may hit multiple SSTables before compaction. Cassandra does use Bloom filters to skip unnecessary SSTables, but read amplification is a known trade-off.
- **Scalability & Availability:** Both scale horizontally, but Cassandra was designed from day one for massive, multi-datacenter clusters with no single point of failure. Mongo can scale with sharding, but Cassandra’s peer-to-peer replication model makes it more fault-tolerant at very large scale.
- **Use Cases:**
    - MongoDB is **better** when you want flexible schemas, ad-hoc queries, and rich secondary indexes.
    - Cassandra is **better** when you need write-heavy, large-scale workloads like time-series data, logging, or event tracking, and when you need to run across multiple datacenters with minimal overhead.
    - On the downside, Cassandra is **worse** for ad-hoc queries or complex analytics without external tools, while MongoDB can handle those more naturally.

---

#### **Pros**
- **Flexible Schema:** NoSQL databases usually work with schema-less or semi-structured data. This makes it easy to store different shapes of data without schema migrations. So you can move fast and adapt the model as your app evolves.
- **Horizontal Scalability:** they are designed to scale out by adding more nodes. In that case, throughput grows almost linearly, so you don’t hit a single bottleneck like in many SQL systems.
- **High Availability:** the distributed, peer-to-peer design ensures there is no single point of failure. Even if some nodes go down, the system keeps running.
- **Performance at Scale:** many NoSQL systems optimize for fast inserts and queries in specific patterns (document lookups, key-value gets, time-series writes). That’s why they are a good fit for web-scale workloads.

<hr class="hr-light" />

#### **Cons**
- **Data Consistency:** many NoSQL databases favor availability and speed over strict consistency. They often use eventual consistency, which means you might read stale data for a short time.
- **Complex Data Modeling:** you need to think about access patterns up front. Whether it’s partition keys, clustering columns, or document shapes, the design defines performance. Bad modeling choices can be hard to fix later.
- **Limited Query Features:** unlike SQL, most NoSQL databases don’t do complex joins or aggregations efficiently. You may need to denormalize or push extra logic into the app or external tools.
- **Operational Overhead:** running large, distributed clusters is not always simple. You need to handle replication, sharding, repair, and monitoring carefully to keep the system reliable.
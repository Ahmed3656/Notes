The **CAP theorem** is a fundamental principle in distributed systems that defines the trade-offs a system designer must make when a network partition occurs. It states that a distributed data store can only provide a strong guarantee for two out of the following three properties: **Consistency**, **Availability**, and **Partition Tolerance**.

#### **Core Concepts**

- **Consistency (C):** this means that every read operation receives the most recent write or an error. It is about the read consistency across all nodes in the system. After a successful write, all subsequent reads, regardless of which node they go to, must return that same value. What I mean is, the system acts like a single, up-to-date copy of the data.
- **Availability (A):** this means that every request (read or write) receives a non-error response, without the guarantee that it contains the most recent write. The system is always operational and will respond, even if the data it returns is stale or outdated.
- **Partition Tolerance (P):** this is the system's ability to continue operating despite an arbitrary number of messages being dropped (or delayed) by the network between nodes. A "partition" is a break in network connectivity that splits the cluster into isolated groups that cannot communicate with each other.

<hr class="hr-light" />

### **The Theorem's Constraint and Trade-Offs**
The core constraint of the CAP theorem is that during a **network partition**, a system must choose between **consistency** and **availability**. You cannot have both. Because network partitions are an inevitable reality in distributed systems, you must design for partition tolerance (P). This leaves two fundamental design choices, defining how a system behaves when a partition occurs.

#### **CP (Consistency over Availability)**
A CP system sacrifices availability to guarantee strong consistency across all operational nodes. The key insight is that it is better to fail a request than to serve stale or inconsistent data. This is often achieved using consensus protocols like **Paxos** or **Raft**.

**How it works:**
- A write operation is only confirmed to the client after it has been replicated and acknowledged by a **quorum** (a majority) of nodes.
- **During a partition** nodes in the minority partition cannot reach a quorum. To avoid serving potentially stale data, they deliberately become unavailable, they will reject writes and may reject reads.
- Meanwhile, nodes in the majority partition can continue operating normally, maintaining a consistent state amongst themselves.

This design ensures that any successful response comes from a consistent view of the system, but at the cost of making some nodes unavailable during a fault.
Real-World CP Systems: **Google Spanner**, **Apache Zookeeper**, **etcd**, **HBase**. These are ideal for scenarios where data correctness is critical, such as financial transactions or cluster coordination.

<hr class="hr-light" />

#### **AP (Availability over Consistency)**
An AP system sacrifices immediate consistency to guarantee maximum availability. The purpose is to keep the system responsive even if parts of it are failing. The philosophy is that a partial or stale response is better than no response at all.

**How it works:**
- A write operation is immediately acknowledged by a single node and then propagated to others asynchronously.
- **During a partition** all nodes remain available and continue to accept reads and writes. However, because they cannot communicate, data diverges. Clients connected to different partitions will see different states of the data, a model often called **eventual consistency**.
- Once the partition heals, the system must reconcile any conflicting writes using mechanisms like **vector clocks**, **last-write-wins**, or application-specific conflict resolution.

This design favors low-latency operations and uptime but requires handling the complexity of temporary inconsistencies.
Real-World AP Systems: **Amazon DynamoDB**, **Apache Cassandra**, **Riak**. These are ideal for use cases where high availability is paramount, such as social media feeds, shopping carts, or product catalogs.

<hr class="hr-light" />

#### **The CA Illusion**
The theorem highlights that a **CA (Consistency & Availability)** system is only logically possible if you assume the absence of network partitions. In practice, because partitions are inevitable in a distributed network, a truly fault-tolerant CA system cannot exist. A single-node database (like a standalone PostgreSQL instance) is CA, but it is not a distributed system, it has no network to partition, making it irrelevant to the theorem's core trade-off. For any distributed data store, the choice is always between CP and AP when a fault occurs.

<hr class="hr-light" />

#### **Tunable Consistency**
Modern systems blur the hard CAP trade-off by letting applications **tune consistency on a per-request basis**. For example:
- **Cassandra** allows choosing from levels like `ONE`, `QUORUM`, `ALL`.
- **Amazon DynamoDB** offers _eventual consistency_ (default) or _strongly consistent reads_.
- **Cosmos DB** provides five levels, from _strong_ to _eventual_.

This flexibility means the same system can behave as **CP or AP depending on the request**. For instance, you might use strong consistency for financial transactions (CP) and eventual consistency for product catalog reads (AP).

---

#### **How to Implement the Choice**
Your implementation choice is often made at the database level, but you can enforce it in your application logic.

- **For a CP-like design:** use a database client that writes with a high consistency level. For example, in Cassandra, you can set a write consistency level of `QUORUM`.  
	Quorum means that a write or read must succeed on a **majority of replicas**. In Cassandra, this is captured by the rule: $R + W > N$
	
	where:
	- `R` = number of replicas required for a successful read
	- `W` = number of replicas required for a successful write
	- `N` = total number of replicas for the data
	
	This ensures that any read will overlap with at least one replica that acknowledged the most recent write, guaranteeing consistency.
    ```bash
    # Writing to Cassandra with QUORUM consistency
    cqlsh> CONSISTENCY QUORUM;
    cqlsh> INSERT INTO users (id, name) VALUES (123, 'name');
    ```
	
    This command will block until a quorum of replicas acknowledges the write. If a partition prevents this, it will time out or return an error, sacrificing availability for consistency.
- **For an AP-like design:** use a lower consistency level for reads and writes to favor speed and availability.
    ```bash
    # Writing to Cassandra with ONE consistency
    cqlsh> CONSISTENCY ONE;
    cqlsh> INSERT INTO users (id, name) VALUES (123, 'name');
    ```
    
    This command will return as soon as a single replica acknowledges the write, making it highly available even if other replicas are partitioned and inconsistent.

---

#### **Pros**
- Provides a simple, powerful mental model for reasoning about distributed systems design and the inherent trade-offs.
- Forces engineers to explicitly consider and choose the behavior of their system under failure conditions, leading to more robust architectures.
- Explains the fundamental design philosophies behind different classes of databases (CP vs. AP).

<hr class="hr-light" />

#### **Cons**
- The "2 of 3" model is often misunderstood and misapplied, leading to the incorrect belief that a CA distributed system is possible.
- It is a very broad theorem; it doesn't prescribe _how_ to achieve CP or AP, leaving the implementation details to the system designer.
- Modern systems often provide tunable knobs (like consistency levels) that allow an application to behave as CP or AP on a per-request basis, making the classification less binary than the theorem suggests.

<hr class="hr-light" />

#### **Clarifications and Common Misconceptions**
- **The "2 of 3" formulation is a simplification.** The choice is not global; it only comes into force _during a network partition_. A well-designed CP system is highly available when there are no partitions, and an AP system can be consistent when the network is healthy.
- **It is not about choosing two to ignore.** Partition Tolerance (P) is not optional in a distributed system; it is a necessity. The real choice is between C and A _when P occurs_.
- **PACELC is an extension.** The PACELC theorem expands on CAP by stating that if there is a Partition (P), the system must choose between Availability and Consistency (A and C); Else (E), when the system is running normally in the absence of partitions, the system must choose between Latency (L) and Consistency (C). This more accurately captures the continuous trade-offs in system design.

---

#### **Real-World Examples**
- **YouTube (Historical AP):** in the mid-2000s, a user updating their profile would write to the primary database master. An immediate page refresh would often route the read to a read replica that had not yet received the update, making it seem like the change was lost. This was an availability choice that led to temporary inconsistency.
- **Financial Banking Systems (CP):** a transfer between accounts must be consistent. The system will lock the accounts and replicate the transaction across nodes before confirming success. If a network partition prevents this consensus, the transaction will fail (become unavailable) rather than risk an inconsistent state like money being deducted from one account but not added to another.
Write amplification happens when a simple, logical “write” ends up triggering far more physical writes than expected. It shows up at multiple layers — the application, the database, and even down at the SSD level — and it can quietly eat away performance, increase latency, and shorten hardware lifespan.

---

#### **Application-Level Write Amplification**
This occurs when a simple user action, like marking a todo item as done, triggers multiple underlying database operations.
- **Example:** updating a todo status might also insert a record into an archive table, increment a counter in a summary table, or update related entities.
- **Cause:** normalized schemas, audit trails, denormalized counters, or soft deletes often require multiple writes for one logical operation.
- **Impact:** a single API call translates into several database writes, increasing load and latency.

<hr class="hr-light" />

**How to Mitigate Application Write Amplification**
- Use denormalization strategically to reduce transactional overhead.
- Batch related writes where possible.
- Consider eventual consistency for non-critical data.
- Avoid over-normalizing tables for high-write workloads.

---

#### **Database-Level Write Amplification**
This is inherent to how database engines manage data and indexes internally.
- **Example (PostgreSQL):** an update to a row creates a new tuple (new row version). All indexes pointing to the old tuple must be updated to point to the new one, even if the indexed columns weren’t changed.
- **Cause:** MVCC (Multi-Version Concurrency Control) implementations and index maintenance.
- **Impact:** a 1-row update with 5 indexes can result in 1 new row + 5 index updates —> 6 physical writes.

<hr class="hr-light" />

**Optimizations**
- **Heap-Only Tuples (HOT) Optimization**
	- PostgreSQL introduced HOT to reduce write amplification. If an updated row fits on the same page and doesn’t change indexed columns, a new tuple is created, but indexes aren’t updated. Instead, the old tuple points to the new one.
	- **Limitation:** Requires free space in the same page. Controlled by the `fillfactor` storage parameter.
	- **Trade-off:** Better update performance vs. potential for page fragmentation.
- **TOAST Tables**
	- Large values are stored out-of-line in TOAST (The Oversized-Attribute Storage Technique) tables. Updating a large value can require rewriting the TOAST entry, adding another layer of write amplification.

**Observability**
- `pg_stat_all_tables` for table update frequency.
- `pg_stat_statements` for high-update queries.
- Autovacuum logs for bloat and cleanup activity.

---

#### **Storage-Level (SSD) Write Amplification**
This occurs due to the fundamental way SSDs handle writes and updates.

**How SSDs handle data**
- Data is stored in pages (4–16KB) grouped into blocks (e.g., 128-256 pages).
- You can write to empty pages, but you **cannot overwrite** a page in-place. An update requires writing the new data to a fresh page and marking the old page as **stale**.
- Garbage collection reclaims blocks by copying valid pages out and then erasing the block.
- **Example:**
	**Initial state**
	Block has 4 pages:
	```text
	Block #1 (before update)
	├── Page A (valid)
	├── Page B (valid)
	├── Page C (valid)
	└── Page D (valid)
	```
	
	**Update Pages B and D**
	SSDs can’t overwrite in place, so:
	- B and D are written as **new versions** (`B'`, `D'`) in a different block.
	- Old B and D become **stale**.
	```text
	Block #2 (new writes)
	├── Page B' (new)
	└── Page D' (new)
	
	Block #1 (after update)
	├── Page A (still valid)
	├── Page B (stale)
	├── Page C (still valid)
	└── Page D (stale)
	```
	
	**Garbage Collection (later on)**
	When the SSD needs free space, Block #1 must be erased. But Block #1 still has **valid data** (A and C). GC must **copy A and C** into a fresh block before erasing Block #1.
	So the total writes triggered are:
	1. Write `B'`, `D'` —> **2 physical writes**.
	2. Copy `A`, `C` during GC —> **2 more physical writes**.
	3. Erase Block #1 —> extra overhead.
	
	**Write Amplification Factor (WAF)**
	- **Logical writes:** 2 (B and D updates).
	- **Physical writes:** 4 (2 updates + 2 copies during GC).
	- **WAF = 4 ÷ 2 = 2.0**
	
	Even though we only updated 2 pages, the SSD did **twice the work** because of stale pages and GC overhead.
	
	## Why This Matters
	- Small, random updates to just part of a block are **worst-case for SSDs**.
	- Databases that do in-place B-Tree updates trigger this pattern constantly.
	- Systems optimized for SSDs (LSM-Trees, append-only logs) avoid this by batching and writing sequentially, so fewer valid pages need to be copied during GC.

<hr class="hr-light" />

- **Write Amplification Factor (WAF):**
    $$WAF = \frac{\text{Physical Writes}}{\text{Logical Writes}}​$$
    Higher WAF = more extra writes —> worse performance and shorter SSD lifespan.
- Garbage collection consumes I/O and CPU, adding latency.
- Wear leveling spreads writes evenly across cells, but frequent updates accelerate wear.

<hr class="hr-light" />

**Impact of B-Trees on SSDs**
- [B-Trees and B+Trees](../Data%20Structures/B-Trees.md) (used by most databases) cause in-place updates during inserts, updates, and rebalancing. This directly contributes to SSD write amplification and garbage collection overhead.

**How to Mitigate SSD Write Amplification**
- Use **append-only storage structures** like [LSM-Trees](../Data%20Structures/LSM%20Tree%20(Log-Structured%20Merge%20Tree).md) (Log-Structured Merge-Trees). They batch writes and perform sequential writes, reducing random updates and stale pages.
- **Examples:** RocksDB, LevelDB, Cassandra, ScyllaDB.
- **Trade-off:** better write performance vs. higher read amplification and compaction overhead.

---

#### **Cross-Layer Effect**
Write amplification is cumulative:
- **Application-level amplification** —> more logical DB writes.
- **Database-level amplification** —> even more internal row/index writes.
- **Storage-level amplification** —> those writes expand further into SSD wear and GC.

One inefficient design choice at the top can cascade down and multiply its cost at the hardware layer.

```text
User Action
   ↓
Multiple DB Writes (App Layer)
   ↓
Row Versions + Index Updates (DB Layer)
   ↓
Extra SSD Pages + GC (Storage Layer)
```

**Numerical Example:**
1. **Application Layer:** 1 logical write —> 3 database writes (due to audit trail, cache update, and counter increment). **WAF = 3**
2. **Database Layer:** 3 database writes —> 15 internal writes (due to MVCC creating new row versions and updating 4 indexes per row). **WAF = 5**
3. **Storage Layer:** 15 internal writes —> 30 physical NAND writes (due to SSD garbage collection copying valid pages). **WAF = 2**
	
	**Cumulative Result:**  
	1 logical write×3×5×2=30 physical writes1 logical write×3×5×2=30 physical writes
	
	One logical write ultimately causes **30x** the physical wear on the storage hardware. This demonstrates why optimizing at just one layer is insufficient.

---

#### **Pros**
- **Understanding write amplification** helps in designing efficient data models and choosing the right database.
- **SSD-aware databases** (using LSM-Trees) can achieve high write throughput and longer SSD lifespan.
- **Optimizations like HOT** in PostgreSQL demonstrate how database engines evolve to reduce internal overhead.

<hr class="hr-light" />

#### **Cons**
- **Complex Debugging:** write amplification is often hidden, making performance issues hard to diagnose.
- **Hardware Stress:** high write amplification reduces SSD lifespan and increases latency during garbage collection.
- **Trade-offs Required:** reducing write amplification (e.g., using LSM-Trees) can increase read amplification and require more CPU for compaction.

---

#### **Real-World Examples**
- **Uber’s Migration from PostgreSQL to MySQL:** in their 2016 engineering post, Uber cited write amplification as a key factor. At that time, PostgreSQL's MVCC implementation required updating all indexes during an UPDATE, regardless of which columns changed. **Note:** This was before HOT optimizations were as mature, and PostgreSQL has significantly improved since then. The example remains valuable for understanding how database-level write amplification impacts real-world systems at scale.
- **LSM-Tree Adoption:** databases like CockroachDB and MongoDB (with WiredTiger) use LSM-like structures to minimize write amplification, especially on SSD-backed storage.
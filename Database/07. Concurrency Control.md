#### **Locking Mechanisms**
Locks are the fundamental tools a database uses to control access to data, ensuring that concurrent transactions don't interfere with each other in harmful ways.

**Exclusive Lock:** a lock that grants a single transaction **exclusive write access** to a piece of data (e.g., a row). It acts as a "Do Not Disturb" sign, preventing all other transactions from reading or writing to the locked data.
- **Purpose:** to ensure no one reads inconsistent or partially updated data during a write operation.
- **Acquired During:** `UPDATE`, `DELETE`, or `INSERT` statements.
- **The Rule:** an exclusive lock **cannot be acquired** if any other lock (shared or exclusive) already exists on the data.
- **Behavior:** while held, it **blocks** all other transactions from acquiring **any type of lock** on the same data.

**Shared Lock:** a lock that allows multiple transactions to **read** the same data simultaneously. It acts as a "Please Do Not Change This" request, preventing writes while allowing concurrent reads.
- **Purpose:** to guarantee read consistency for the duration of a transaction (e.g., ensuring a balance doesn't change during a long report).
- **Acquired During:** `SELECT` statements (often explicitly with `FOR SHARE` or under certain transaction isolation levels).
- **The Rule:** a shared lock **cannot be acquired** if an exclusive lock is already held on the data.
- **Behavior:** while held, it **allows** other shared locks but **blocks** exclusive locks from being acquired.

---

#### **Concurrency Control Protocols**
These protocols define the rules for how and when locks are acquired and released to manage concurrent transactions safely.

**Two-Phase Locking (2PL):** A concurrency control protocol that **guarantees serializability** by forcing transactions to handle locks in **two clear phases**:
- **Growing Phase** —> You can **only acquire** locks (shared or exclusive).
- **Shrinking Phase** —> You can **only release** locks.
The **core rule**: once you release a lock, you can’t acquire new ones. This separation prevents transactions from messing with each other and keeps data consistent.

**Types of 2PL:**
1. **Basic 2PL** (Rarely used)
    - Locks can be released **anytime** during the shrinking phase.
    - Risky because **cascading rollbacks** can occur if a transaction releases a lock too early and another transaction reads uncommitted data.
2. **Strict 2PL** (Most common, used by MySQL & PostgreSQL)
    - **Write locks are held until the transaction commits or rolls back.**
    - Ensures that no other transaction can **read or write** data that might still be rolled back later.
    - Prevents cascading rollbacks and ensures **stronger consistency**.
3. **Rigorous 2PL** (Used in critical systems)
    - **Both read and write locks** are held until the transaction finishes.
    - Even safer than Strict 2PL but comes at the cost of **lower concurrency**.
    - Usually used in **banking, finance, and high-integrity systems** where data correctness outweighs performance.

**Why It Exists:** 
- Without 2PL, you risk race conditions. Example: two people book the same cinema seat at the same time. If you **check availability** and then **update**, both can see the seat as “free” and both try to book it, boom, **double-booking**.
- With 2PL, the first transaction locks the seat immediately, so anyone else trying to book must **wait**. By the time they get the lock, they’ll see the updated state and know the seat’s gone. Problem solved.

**Practical Use Case (The Booking Problem):**

Imagine an **online cinema seat booking system**
- **The Wrong Way (Without Locks):**
    1. Transaction A checks seat #10 —> sees it’s free.
    2. Transaction B checks seat #10 —> also sees it’s free.
    3. Both try to update —> **double-booking** happens.
- **The 2PL Way:**
    1. Transaction A **acquires an exclusive lock** on seat #10.
    2. Transaction B tries to book —> **forced to wait**.
    3. A commits —> lock released —> B now sees the updated state.
This ensures only **one person** books the seat, no race conditions, no data corruption.
##### **The Pizza Analogy**
Think of the **last slice of pizza**:
- **Risky Way (Update-and-Check):**  
    You yell from the kitchen, “I’m taking it!” but don’t grab it yet.
    - In SQL:
	    ```sql
	    UPDATE seats 
		SET is_booked = 1 
		WHERE seat_id = 1 AND is_booked = 0;
	    ```
	    
	You **rely on the DB** to handle conflicts automatically. It _usually_ works, but you’re trusting hidden behavior.
    
- **Safe Way (SELECT FOR UPDATE):**  
    You walk up, **put your hand on the slice**, and then announce, “This is mine.”
    - In SQL:
	    ```sql
	    SELECT * FROM seats 
		WHERE seat_id = 1 
		FOR UPDATE;
	    ```
		
	This **explicitly locks** the row, guaranteeing no one else can touch it until you commit.
	
**Takeaway:** For critical operations like bookings or payments, **explicit locks** are the professional, bulletproof approach.

**How Databases Handle This:**
- When you lock a row, other transactions **queue behind you**.
- First transaction —> updates —> commits —> releases lock.
- Waiting transactions —> resume —> re-check data before proceeding.

From the **user’s perspective**, the app might seem like it’s **“loading”**, but it’s simply **waiting for your transaction to finish**.  
To improve UX, systems often set **timeouts**:
> “Seat is being booked by another user, try again later.”

**Trade-off:** while it ensures strict data consistency, it can **reduce concurrency** and increase the chance of **deadlocks**, as transactions may hold locks for longer periods while waiting for other operations to complete.
    - **Concurrency Cost:** the direct cost of 2PL is **reduced concurrency**. While one transaction holds the lock (even for milliseconds), all other transactions targeting the same resource are blocked. This is the necessary price for absolute consistency.
    - **The "Pessimistic" Mindset:** this approach is called **pessimistic locking** because it assumes conflicts will be common, so it prevents them by locking resources upfront.

<hr class="hr-light" />

**Deadlocks:** a specific failure scenario where two or more database transactions are permanently stuck, each waiting for the other to release a lock on a resource before it can proceed.
- **The Cause:** it occurs due to a **circular dependency** on locks. For example:
    - Transaction A holds a lock on Resource 1 and requests a lock on Resource 2.
    - Transaction B holds a lock on Resource 2 and requests a lock on Resource 1.
    - Each transaction is waiting for the other to release the lock it needs, resulting in a permanent standstill.
- **Automatic Detection:** modern databases (like PostgreSQL, MySQL) have a **deadlock detector** that automatically identifies these circular waits.
- **The Resolution:** the database **forcefully aborts (rolls back)** one of the transactions involved in the deadlock. This breaks the cycle and allows the other transaction(s) to complete.
    - The transaction chosen to be aborted is typically the one that is deemed the easiest to roll back, often the one that entered the deadlock situation last.
- **Client Impact:** the aborted transaction will receive an error, and the application must be designed to **retry** the transaction from the beginning.
- **Common Scenario:** often happens with concurrent `INSERT`, `UPDATE`, or `DELETE` operations on multiple tables or rows in a different order by different transactions. The example in the transcript shows a deadlock on primary key values during concurrent inserts.

##### **How to Prevent Deadlocks**
1. **Always lock resources in a consistent order:** if two transactions update `users` then `orders`, **always** lock `users` first.
2. **Keep transactions short & simple:** the longer you hold locks, the higher the deadlock risk.
3. **Use smaller lock scopes:** prefer **row-level locks** instead of **table locks** when possible.
4. **Enable timeout & retry logic:** set a transaction timeout (e.g., `LOCK_TIMEOUT = 5s`) and **retry on deadlock**.

---

#### **Performance & Efficiency Patterns**
These patterns address common bottlenecks in database-driven applications, focusing on optimizing data retrieval and connection management.

**SQL OFFSET:** a SQL clause used to skip a specified number of rows before starting to return the result set of a query. It is the traditional tool for implementing pagination in applications.
- **Core Function:** `OFFSET N` instructs the database to **fetch and then discard** the first `N` rows from the sorted result set before returning any rows to the client.
- **The Performance Problem:** its major flaw is that the cost of using `OFFSET` scales linearly with the number of rows skipped. To serve page 1,000 (e.g., `OFFSET 10000`), the database must physically read, sort, and discard the first 10,000 rows before it can return the desired 10 results. This becomes extremely slow and resource-intensive for deep pagination.
- **The Consistency Problem:** `OFFSET` can lead to **duplicate or missing records** in a dynamic dataset. If a new record is inserted into a table between a user viewing page 1 and page 2, the entire result set shifts. This means the last record on page 1 will appear again as the first record on page 2, and one record will be missed entirely.
- **The Alternative ("Keyset Pagination"):** the efficient solution is to avoid `OFFSET` and instead paginate using a **unique, sequential filter** (like an auto-incrementing ID or a timestamp).
    - **How it works:** instead of asking for "page 10," the client asks for "the next 10 records _after_ the last ID I saw." The query uses a `WHERE` clause (e.g., `WHERE id < {last_seen_id}`) to efficiently seek to the correct starting point in the index, then applies a `LIMIT`.
    - **Why it's better:** the database uses an index to jump directly to the starting point, reading only the `LIMIT` number of rows. This makes performance constant and fast, regardless of how deep into the result set the user goes. It also remains stable against concurrent inserts or deletions.
```sql
-- Inefficient OFFSET pagination
SELECT * FROM users
ORDER BY id
LIMIT 10 OFFSET 10000;

-- Efficient Keyset Pagination
SELECT * FROM users
WHERE id > 10000
ORDER BY id
LIMIT 10;
```

**Why it’s better:**
- Uses **indexes** directly —> **constant time**, even for deep pages.
- Avoids duplicates/missing records during concurrent inserts/deletes.

<hr class="hr-light" />

**Database Connection Pooling:** a design pattern that maintains a cache (or "pool") of pre-established, reusable database connections. This allows multiple clients in an application to share these connections, rather than each client creating and destroying its own connection for every database request.
- **Core Purpose:** to eliminate the significant overhead and latency associated with repeatedly establishing and tearing down database connections. This overhead includes:
    - **Network Latency:** the TCP three-way handshake required to establish a connection.
    - **Authentication:** the protocol handshake where the client authenticates with the database server.
    - **Resource Allocation:** the server and client OS resources (memory, file descriptors) allocated per connection.
- **How it Works:** on application startup, the pool creates a fixed number of connections (`max`) and keeps them in an idle state. When a client (e.g., a web server handling an API request) needs to run a query, it **checks out** a connection from the pool, uses it, and then **returns it** to the pool for reuse. It does not close the underlying TCP connection.
- **Key Configuration Parameters:**
    - **`max`:** the maximum number of connections the pool is allowed to create. This is critical to avoid overwhelming the database server with more connections than it can handle.
    - **`connectionTimeoutMillis`:** how long a client request should wait for a connection to become available from the pool if all `max` connections are currently in use. A value of `0` means wait indefinitely.
    - **`idleTimeoutMillis`:** how long an idle connection is allowed to sit in the pool before it is automatically closed and its resources are freed. This prevents memory bloat from unused connections.
- **Performance Impact:** connection pooling provides a significant performance boost by eliminating repeated connection setup costs. Benchmarks comparing 1000 queries show the "old way" (open/close per request) averaging **~40ms** per operation, while the pooled approach averages **~19ms**, a **>50% improvement**. This performance gap widens substantially with the added network latency of a remote cloud database, making pooling essential for production applications.
- **The "Old Way" (Anti-Pattern):** the stateless approach of opening a new connection for every HTTP request and closing it immediately after the query completes is highly inefficient. While it works, it needlessly burns CPU cycles and network bandwidth on connection setup/teardown, leading to higher latency and lower overall application throughput.
- **Advanced Use Case (Transactions):** for operations that require ACID transactions (multiple queries that must succeed or fail together), a client can **check out a dedicated connection** from the pool. This "locks" that specific connection for the duration of the transaction, ensuring all statements are executed on the same session before the connection is released back to the pool.

##### **How Connection Pooling Works**
```pgsql
[ Application Request ] —> [ Check Pool ]
         ↓
[ Idle Connection? ] ─ Yes —> [ Reuse It ]
         ↓ No
[ Pool Full? ] ─ Yes —> [ Wait / Timeout ]
         ↓ No
[ Create New Connection ]
         ↓
[ Execute Query ] —> [ Return Connection to Pool ]
```

**Tuning Tips**
- **Too small pool —> requests queue, higher latency.
- **Too large pool —>** database gets overloaded.
- **Rule of Thumb:** `(DB cores × 2) + Effective concurrency` —> optimal max pool size.

---

#### **Notes**
- **Lock Lifecycle in Transactions**: when you update a row, the database grabs an exclusive lock on it. That lock stays in place for the entire transaction and only goes away after a `COMMIT` or `ROLLBACK`. The purpose is to ensure isolation, no other transaction can touch the same row while yours is open. Different databases may use row-level or table-level locks, but they all follow the same principle: once you lock inside a transaction, you hold it until the very end.
- **Write Amplification is a critical performance penalty** where a single logical write (e.g., an `UPDATE`) forces multiple physical I/O operations.
    - **Cause (in PostgreSQL):** MVCC creates a new row version (tuple) on `UPDATE`. Every index must update its pointer to the new tuple ID, amplifying one write into many.
    - **Consequence:** high I/O load and accelerated SSD wear.
- **HOT (Heap Only Tuple) is PostgreSQL's optimization** to avoid write amplification from index updates.
    - **How it works:** if the new tuple fits on the same page as the old one, it creates it there and adds an internal pointer. Indexes keep pointing to the old tuple, and the pointer is followed on read.
    - **Critical limitation:** only works if the update doesn’t increase row size and the page has free space. Otherwise, it falls back to updating all indexes.
- **Fill Factor is a storage parameter** that controls free space left on database pages during initial load.
    - **Purpose:** to reserve space for HOT updates. A lower fill factor (e.g., `80`) leaves more free space, making HOT more likely.
    - **Trade-off:** improves update performance but increases storage size and can slightly slow full scans.
- **Page-Level Locking & Insert Contention:** when using sequential keys (auto-increment, ULIDs) on clustered indexes, all concurrent inserts target the same "last page," causing latch contention and serialized writes. Mitigations include page splitting, fill factor optimization, and buffer pool management, but this represents a fundamental trade-off: sequential keys enable excellent read locality at the cost of write contention, while random keys distribute writes but sacrifice read performance and buffer pool efficiency.